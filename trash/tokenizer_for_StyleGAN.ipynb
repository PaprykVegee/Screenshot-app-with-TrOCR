{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\patry\\NLP_app')\n",
    "\n",
    "from image_text_generator import ImageTextGenerator\n",
    "\n",
    "generator = ImageTextGenerator(model_name=\"flax-community/papuGaPT2\", fonts_folder=r\"C:\\Users\\patry\\Desktop\\wypakwoane\", random_state=42)\n",
    "df = generator.generate_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bag = df['text'].explode().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'put', 'down', 'a', 'res', '##ol', '##ution', '[SEP]']\n",
      "Tokenizer został zapisany w lokalizacji: C:/Users/patry/NLP_app/tokenizer_for_GAN\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents, Replace\n",
    "from tokenizers.pre_tokenizers import Whitespace, Punctuation\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import tokenizers\n",
    "import os\n",
    "\n",
    "# Tworzenie modelu tokenizacji\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n",
    "\n",
    "# Ustawienie normalizera z poprawnym użyciem tokenizers.Regex\n",
    "tokenizer.normalizer = Sequence([\n",
    "    # Usuwanie znaków specjalnych, ale nie białych znaków\n",
    "    Replace(tokenizers.Regex(r\"[\\p{Other}&&[^\\n\\t\\r]]\"), \"\"),  \n",
    "    Lowercase(),\n",
    "    NFD(),\n",
    "    StripAccents()\n",
    "])\n",
    "\n",
    "# Ustawienie pre-tokenizera\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    Whitespace(),  # Dzieli po białych znakach\n",
    "    Punctuation()  # Dzieli znaki interpunkcyjne\n",
    "])\n",
    "\n",
    "# Ustawienie trenera\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = WordPieceTrainer(vocab_size=1702, special_tokens=special_tokens)\n",
    "\n",
    "# Trening tokenizera\n",
    "tokenizer.train_from_iterator(text_bag, trainer=trainer)\n",
    "\n",
    "# Pobieranie identyfikatorów specjalnych tokenów\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "# Ustawienie post-processora\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single = f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair = f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens = [(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")\n",
    "\n",
    "# Ustawienie dekodera\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "# Testowanie tokenizera\n",
    "encoded = tokenizer.encode(\"put down a resolution\")\n",
    "print(encoded.tokens)  # Powinno wyświetlić tokeny\n",
    "\n",
    "# Zapisanie tokenizera do pliku w podanej lokalizacji\n",
    "save_path = \"C:/Users/patry/NLP_app/tokenizer_for_GAN\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Tworzy katalog, jeśli nie istnieje\n",
    "tokenizer.save(save_path)\n",
    "\n",
    "print(f\"Tokenizer został zapisany w lokalizacji: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'put', 'down', 'a', 'res', '##ol', '##ution', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Ścieżka do pliku z zapisanym tokenizatorem\n",
    "load_path = \"C:/Users/patry/NLP_app/tokenizer_for_GAN\"\n",
    "\n",
    "# Importowanie tokenizatora z pliku\n",
    "tokenizer = Tokenizer.from_file(load_path)\n",
    "\n",
    "# Testowanie zaimportowanego tokenizatora\n",
    "encoded = tokenizer.encode(\"put down a resolution\")\n",
    "print(encoded.tokens)  # Wyświetla zakodowane tokeny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokeny (put down a resolution): ['[CLS]', 'put', 'down', 'a', 'res', '##ol', '##ution', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Tokeny (this is a longer example...): ['[CLS]', 'this', 'is', 'a', 'long', '##er', 'ex', '##amp', '##le', 'sent', '##ence', 'that', 'need', '##s', 'tr', '##un', '##c', '##ation', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Tokeny (short): ['[CLS]', 'short', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Długość (put down a resolution): 100\n",
      "Długość (this is a longer example...): 100\n",
      "Długość (short): 100\n"
     ]
    }
   ],
   "source": [
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\", length=100)\n",
    "#tokenizer.enable_truncation(max_length=10)  # Maksymalna długość sekwencji ustawiona na 10\n",
    "\n",
    "# Testowanie tokenizera na różnych długościach tekstu\n",
    "encoded_1 = tokenizer.encode(\"put down a resolution\")\n",
    "encoded_2 = tokenizer.encode(\"this is a longer example sentence that needs truncation\")\n",
    "encoded_3 = tokenizer.encode(\"short\")\n",
    "\n",
    "print(f\"Tokeny (put down a resolution): {encoded_1.tokens}\")\n",
    "print(f\"Tokeny (this is a longer example...): {encoded_2.tokens}\")\n",
    "print(f\"Tokeny (short): {encoded_3.tokens}\")\n",
    "\n",
    "print(f\"Długość (put down a resolution): {len(encoded_1.tokens)}\")\n",
    "print(f\"Długość (this is a longer example...): {len(encoded_2.tokens)}\")\n",
    "print(f\"Długość (short): {len(encoded_3.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 205,\n",
       " 152,\n",
       " 31,\n",
       " 583,\n",
       " 100,\n",
       " 212,\n",
       " 605,\n",
       " 132,\n",
       " 918,\n",
       " 290,\n",
       " 141,\n",
       " 594,\n",
       " 74,\n",
       " 243,\n",
       " 298,\n",
       " 75,\n",
       " 177,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_2.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
